{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cffb8a-c0f2-40bd-84e7-7b5a0bd7f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pickle import dump\n",
    "from sklearn import preprocessing\n",
    "from datetime import datetime\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from pykalman import KalmanFilter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset_path = os.getcwd() +'/data/'# Initial folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5a1b207b-5cdd-47b7-900e-fe49c893674a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2343629206.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[117], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    svn checkout https://github.com/hunissweet/ML_2/tree/main/data_set\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Py_KOOC/ML1/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c47b24da-a3af-4b62-beb4-ea8b378e40d2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating one\n",
      "Download the data\n",
      "unzipping the data\n"
     ]
    }
   ],
   "source": [
    "# setup path to a data folder\n",
    "image_path= Path(dataset_path)\n",
    "\n",
    "if image_path.is_dir():\n",
    "    print(f'{image_path} directory is already existed ... skipping download')\n",
    "else:\n",
    "    print('creating one')\n",
    "    image_path.mkdir(parents=True,exist_ok=True)\n",
    "    # download the data\n",
    "    with open(image_path /'data.zip','wb') as f: # wb: write binary\n",
    "        request=requests.get('https://github.com/hunissweet/Begin_SOMARO/raw/main/data.zip')\n",
    "        \n",
    "        print('Download the data')\n",
    "        f.write(request.content)\n",
    "\n",
    "    with zipfile.ZipFile(image_path/'data.zip','r') as zip_ref:\n",
    "        print('unzipping the data')\n",
    "        zip_ref.extractall(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5244490a-6561-461d-9fde-188aa73b8022",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path='data/'  #new data set path after un zipping the file\n",
    "train_data_dir = dataset_path + 'train' # Folder for original data\n",
    "test_data_dir  = dataset_path + 'test'  # Folder for original data \n",
    "train_out_dir  = dataset_path + 'train_out' # After formmating\n",
    "test_out_dir   = dataset_path + 'test_out'  # After formmating\n",
    "scaler_out_dir = dataset_path + 'scale_er'  # pickle file for scaler => This file is processor for scaling, such as min-max, standarziation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "33fb4ff7-f53b-40eb-91b5-88da8ce6f438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir path : data/train\n",
      "number of dir name : 5\n",
      "Filenames []\n",
      "Dir path : data/train\\data_sample_2022-02-21-09-58-49\n",
      "number of dir name : 0\n",
      "Filenames ['marker.csv', 'meta_data.csv', 'robot_state.csv', 'slip_label.csv', 'xela_sensor1.csv', 'xela_sensor2.csv']\n",
      "\n",
      "\n",
      "Dir path : data/test\n",
      "number of dir name : 2\n",
      "Filenames []\n",
      "Dir path : data/test\\data_sample_2022-02-22-11-23-11\n",
      "number of dir name : 0\n",
      "Filenames ['marker.csv', 'meta_data.csv', 'robot_state.csv', 'slip_label.csv', 'xela_sensor1.csv', 'xela_sensor2.csv']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def walk_through_dir(dir_path):\n",
    "    for dirpath,dirnames,filenames in os.walk(dir_path):\n",
    "        print(f'Dir path : {(dirpath)}\\nnumber of dir name : {len(dirnames)}\\nFilenames {filenames}')\n",
    "        if len(filenames)>0:\n",
    "            print('\\n')\n",
    "            break\n",
    "walk_through_dir(train_data_dir)\n",
    "\n",
    "walk_through_dir(test_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b24fa95e-80a6-4a2e-80ec-906e29f9ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyper parameter ###\n",
    "smooth = False\n",
    "filter_kalman = True\n",
    "image = False\n",
    "image_height = 64\n",
    "image_width = 64\n",
    "context_length = 10\n",
    "horrizon_length = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "370277e9-dd2b-46ff-9e75-27e5d937057f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:03<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train_out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:12,  2.43s/it]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train_out/map.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 158\u001b[0m\n\u001b[0;32m    154\u001b[0m     df\u001b[38;5;241m.\u001b[39mcreate_map()\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[125], line 154\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    152\u001b[0m df\u001b[38;5;241m.\u001b[39mload_file_names()\n\u001b[0;32m    153\u001b[0m df\u001b[38;5;241m.\u001b[39mscale_data()\n\u001b[1;32m--> 154\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[125], line 74\u001b[0m, in \u001b[0;36mdata_formatter.create_map\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_no \u001b[38;5;241m=\u001b[39m experiment_number\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_map(path_save, test\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_save\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[125], line 84\u001b[0m, in \u001b[0;36mdata_formatter.save_map\u001b[1;34m(self, path, test)\u001b[0m\n\u001b[0;32m     82\u001b[0m             writer\u001b[38;5;241m.\u001b[39mwriterow(row)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/map.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n\u001b[0;32m     85\u001b[0m         writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(csvfile, quoting\u001b[38;5;241m=\u001b[39mcsv\u001b[38;5;241m.\u001b[39mQUOTE_ALL)\n\u001b[0;32m     86\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwriterow([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrobot_data_path\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtactile_data_sequence\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment_number\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_steps\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailure\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ml_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train_out/map.csv'"
     ]
    }
   ],
   "source": [
    "class data_formatter:\n",
    "    def __init__(self):\n",
    "        self.files_train = []\n",
    "        self.files_test = []\n",
    "        self.full_data_tactile = []\n",
    "        self.full_data_robot = []\n",
    "        self.smooth = smooth\n",
    "        self.filter_kalman = filter_kalman\n",
    "        self.context_length = context_length\n",
    "        self.horrizon_length = horrizon_length\n",
    "\n",
    "    def create_map(self):\n",
    "        for stage in [train_out_dir, test_out_dir]:\n",
    "            self.path_file = []\n",
    "            index_to_save = 0\n",
    "            print(stage)\n",
    "            if stage == train_out_dir:\n",
    "                files_to_run = self.files_train\n",
    "            else:\n",
    "                files_to_run = self.files_test\n",
    "\n",
    "            for experiment_number, file in tqdm(enumerate(files_to_run)):\n",
    "                if stage == test_out_dir:\n",
    "                    path_save = stage + \"/test_trial_\" + str(experiment_number) + '/'\n",
    "                    os.mkdir(path_save)\n",
    "                    self.path_file = []\n",
    "                    index_to_save = 0\n",
    "                else:\n",
    "                    path_save = stage\n",
    "\n",
    "                tactile, robot, meta, slip, failure = self.load_file_data(file)\n",
    "\n",
    "                # scale the data\n",
    "                for index, (standard_scaler, min_max_scalar) in enumerate(zip(self.tactile_standard_scaler, self.tactile_min_max_scalar)):\n",
    "                    tactile[:, index] = standard_scaler.transform(tactile[:, index])\n",
    "                    tactile[:, index] = min_max_scalar.transform(tactile[:, index])\n",
    "                \n",
    "                for index, min_max_scalar in enumerate(self.robot_min_max_scalar):\n",
    "                    robot[:, index] = np.squeeze(min_max_scalar.transform(robot[:, index].reshape(-1, 1)))\n",
    "                \n",
    "\n",
    "                sequence_length = self.context_length + self.horrizon_length\n",
    "                for time_step in range(len(tactile) - sequence_length):\n",
    "                    robot_data_sequence = [robot[time_step + t] for t in range(sequence_length)]\n",
    "                    tactile_data_sequence     = [tactile[time_step + t] for t in range(sequence_length)]\n",
    "                    experiment_data_sequence  = experiment_number\n",
    "                    time_step_data_sequence   = [time_step + t for t in range(sequence_length)]\n",
    "                    slip_label_sequence = slip[time_step + sequence_length - 1]\n",
    "                    failure_label_sequence = failure[time_step + sequence_length - 1]\n",
    "                  \n",
    "                    ###################################### Save the data and add to the map ###########################################\n",
    "                    np.save(path_save + 'robot_data_' + str(index_to_save), robot_data_sequence)\n",
    "                    np.save(path_save + 'tactile_data_sequence_' + str(index_to_save), tactile_data_sequence)\n",
    "                    np.save(path_save + 'experiment_number_' + str(index_to_save), experiment_data_sequence)\n",
    "                    np.save(path_save + 'time_step_data_' + str(index_to_save), time_step_data_sequence)\n",
    "                    np.save(path_save + 'trial_meta_' + str(index_to_save), np.array(meta))\n",
    "                    np.save(path_save + 'slip_data_' + str(index_to_save), np.array(slip_label_sequence))\n",
    "                    np.save(path_save + 'failure_data_' + str(index_to_save), np.array(failure_label_sequence))\n",
    "                    ref = []\n",
    "                    ref.append('robot_data_' + str(index_to_save) + '.npy')\n",
    "                    ref.append('tactile_data_sequence_' + str(index_to_save) + '.npy')\n",
    "                    ref.append('experiment_number_' + str(index_to_save) + '.npy')\n",
    "                    ref.append('time_step_data_' + str(index_to_save) + '.npy')\n",
    "                    ref.append('trial_meta_' + str(index_to_save) + '.npy')\n",
    "                    ref.append('slip_data_' + str(index_to_save) + '.npy')\n",
    "                    ref.append('failure_data_' + str(index_to_save) + '.npy')\n",
    "                    self.path_file.append(ref)\n",
    "                    index_to_save += 1\n",
    "\n",
    "                if stage == test_out_dir:\n",
    "                    self.test_no = experiment_number\n",
    "                    self.save_map(path_save, test=True)\n",
    "\n",
    "            self.save_map(path_save)\n",
    "\n",
    "    def save_map(self, path, test=False):\n",
    "        if test:\n",
    "            with open(path + '/map_' + str(self.test_no) + '.csv', 'w') as csvfile:\n",
    "                writer = csv.writer(csvfile, quoting=csv.QUOTE_ALL)\n",
    "                writer.writerow(['robot_data_path', 'tactile_data_sequence', 'experiment_number', 'time_steps', 'meta', 'slip', 'failure'])\n",
    "                for row in self.path_file:\n",
    "                    writer.writerow(row)\n",
    "        else:\n",
    "            with open(path + '/map.csv', 'w') as csvfile:\n",
    "                writer = csv.writer(csvfile, quoting=csv.QUOTE_ALL)\n",
    "                writer.writerow(['robot_data_path', 'tactile_data_sequence', 'experiment_number', 'time_steps', 'meta', 'slip', 'failure'])\n",
    "                for row in self.path_file:\n",
    "                    writer.writerow(row)\n",
    "\n",
    "    def scale_data(self):\n",
    "        files = self.files_train + self.files_test\n",
    "        for file in tqdm(files):\n",
    "            tactile, robot, _, _, _ = self.load_file_data(file)\n",
    "            self.full_data_tactile += list(tactile)\n",
    "            self.full_data_robot += list(robot)\n",
    "\n",
    "        self.full_data_tactile = np.array(self.full_data_tactile)\n",
    "        self.full_data_robot = np.array(self.full_data_robot)\n",
    "\n",
    "        self.robot_min_max_scalar = [preprocessing.MinMaxScaler(feature_range=(0, 1)).fit(self.full_data_robot[:, feature].reshape(-1, 1)) for feature in range(2)]\n",
    "        self.tactile_standard_scaler = [preprocessing.StandardScaler().fit(self.full_data_tactile[:, feature]) for feature in range(3)]\n",
    "        self.tactile_min_max_scalar = [preprocessing.MinMaxScaler(feature_range=(0, 1)).fit(self.tactile_standard_scaler[feature].transform(self.full_data_tactile[:, feature])) for feature in range(3)]\n",
    "\n",
    "        self.save_scalars()\n",
    "\n",
    "    def load_file_data(self, file):\n",
    "        robot_state = np.array(pd.read_csv(file + '/robot_state.csv', header=None))\n",
    "        meta_data = np.array(pd.read_csv(file + '/meta_data.csv', header=None))\n",
    "        xela_sensor = pd.read_csv(file + '/xela_sensor1.csv')\n",
    "        slip_label = pd.read_csv(file + '/slip_label.csv')['slip']\n",
    "        fail_label = pd.read_csv(file + '/slip_label.csv')['fail']\n",
    "\n",
    "        xela_sensor = np.array(xela_sensor.sub(xela_sensor.loc[0]))\n",
    "\n",
    "        robot_task_space = np.array([[state[-3], state[-2]] for state in robot_state[1:]]).astype(float)\n",
    "\n",
    "        tactile_data_split = [np.array(xela_sensor[:, [i for i in range(feature, 48, 3)]]).astype(float) for feature in range(3)]\n",
    "        tactile_data = np.array([[tactile_data_split[feature][ts] for feature in range(3)] for ts in range(tactile_data_split[0].shape[0])]) #shape: lenx3x16\n",
    "       \n",
    "        if self.filter_kalman:\n",
    "            for i in range(tactile_data.shape[1]):\n",
    "                if i < 2: # x, y initial cov = 4\n",
    "                    kf = KalmanFilter(initial_state_mean=tactile_data[0, i, :], n_dim_obs=16, initial_state_covariance=1*np.eye(16), observation_covariance=4*np.eye(16))\n",
    "                    tactile_data[:, i, :], _ = kf.filter(tactile_data[:, i, :])\n",
    "                elif i ==2: # z initial cov = 8\n",
    "                    kf = KalmanFilter(initial_state_mean=tactile_data[0, i, :], n_dim_obs=16, initial_state_covariance=1*np.eye(16), observation_covariance=8*np.eye(16))\n",
    "                    tactile_data[:, i, :], _ = kf.filter(tactile_data[:, i, :])\n",
    "        \n",
    "\n",
    "        return tactile_data, robot_task_space, meta_data, slip_label, fail_label\n",
    "\n",
    "    def load_file_names(self):\n",
    "        self.files_train = glob.glob(train_data_dir + '/*')\n",
    "        self.files_test = glob.glob(test_data_dir + '/*')\n",
    "\n",
    "    def save_scalars(self):\n",
    "        # save the scalars\n",
    "        dump(self.tactile_standard_scaler[0], open(scaler_out_dir + 'tactile_standard_scaler_x.pkl', 'wb'))\n",
    "        dump(self.tactile_standard_scaler[1], open(scaler_out_dir + 'tactile_standard_scaler_y.pkl', 'wb'))\n",
    "        dump(self.tactile_standard_scaler[2], open(scaler_out_dir + 'tactile_standard_scaler_z.pkl', 'wb'))\n",
    "        dump(self.tactile_min_max_scalar[0], open(scaler_out_dir + 'tactile_min_max_scalar_x.pkl', 'wb'))\n",
    "        dump(self.tactile_min_max_scalar[1], open(scaler_out_dir + 'tactile_min_max_scalar_y.pkl', 'wb'))\n",
    "        dump(self.tactile_min_max_scalar[2], open(scaler_out_dir + 'tactile_min_max_scalar_z.pkl', 'wb'))\n",
    "\n",
    "\n",
    "        dump(self.robot_min_max_scalar[0], open(scaler_out_dir + 'robot_min_max_scalar_vx.pkl', 'wb'))\n",
    "        dump(self.robot_min_max_scalar[1], open(scaler_out_dir + 'robot_min_max_scalar_vy.pkl', 'wb'))\n",
    "\n",
    "\n",
    "def main():\n",
    "    df = data_formatter()\n",
    "    df.load_file_names()\n",
    "    df.scale_data()\n",
    "    df.create_map()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3336f69-a8b4-47e6-843b-831d705fde31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello github\n"
     ]
    }
   ],
   "source": [
    "# NEW line for testing GITHUB\n",
    "print('Hello github')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fff4c2e-e923-41c2-abd3-4a9a7917be6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
